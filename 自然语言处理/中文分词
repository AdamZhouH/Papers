> 目前的中文分主要有两种思路
> 1.基于文本匹配，也即是去匹配一个给定的充分大的词典进行词条匹配
> 2.基于统计的机器学习算法


## 现存可用的开源分词工具
### 1.jieba中文分词
> 这个是一个开源中文中文分词工具，三种中文分词的模式，适用于不同场景包括
- 精确模式 (试了下，这个模式的识别会效果好些)
- 全模式
- 搜索引擎模式
> 可以自定义词典，提高针对法律领域内的中文分词的准确性，但是识别的准确程度的具体数据没有给出
> 处理Unicode编码
> 速度在几个工具中是最快的，准确率也比较高

### 2.THULAC 清华大学开源的一款中文分词工具
> 提供分词和词性标注
> 分词结果的F1率达到97.3%，同时速度较快

### 3.MMSEG 采用MMSEG算法实现的中文分词工具
> 算法有两种分词方法，简单和复杂
> 都是基于正向最大匹配原则，复杂模式下，加入了四个过滤规则从而解决部分的分词歧义
> 具体实现有 LibMMSeg 和 friso 等
